{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef0de40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " The Zillow csv provides ZHVI for neighborhoods in the US.\n",
    " Zillow Home Value Index (ZHVI): A measure of the typical home value and market changes across a given region and housing type. \n",
    " It reflects the typical value for homes in the 35th to 65th percentile range.\n",
    " More info about ZHVI: https://www.zillow.com/research/methodology-neural-zhvi-32128/\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('Neighborhood_zillow.csv')\n",
    "Neighborhoods_zillow_df = df.copy()\n",
    "print(\"shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b90ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print columns\n",
    "print(\"Columns:\")\n",
    "print(df.columns.to_list())\n",
    "\n",
    "chicago_df = df[df['City'] == \"Chicago\"]\n",
    "\n",
    "# Different neighborhoods\n",
    "print()\n",
    "print(\"Chicago neighborhoods:\")\n",
    "print(chicago_df['RegionName'].unique()[:15])\n",
    "\n",
    "# isolate Little Italy\n",
    "print()\n",
    "little_italy = chicago_df[chicago_df['RegionName'] == 'University Village - Little Italy']\n",
    "print(\"Little Italy 2000-01\", little_italy['2000-01-31'])\n",
    "print(\"Little Italy 2024-09\", little_italy['2024-09-30'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b948905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter regions into different dataframes based on classification\n",
    "\n",
    "# North Side\n",
    "north_side_regions = [\n",
    "    'Rogers Park', 'Edgewater', 'Uptown', 'Lake View', 'Lincoln Park', \n",
    "    'North Center', 'Lincoln Square', 'West Ridge', 'Irving Park', \n",
    "    'Albany Park', 'Avondale'\n",
    "]\n",
    "north_side_df = chicago_df[chicago_df['RegionName'].isin(north_side_regions)]\n",
    "\n",
    "# South Side\n",
    "south_side_regions = [\n",
    "    'Armour Square', 'Bridgeport', 'Brighton Park', 'New City (Back of the Yards)', \n",
    "    'Englewood', 'Greater Grand Crossing', 'Hyde Park', 'Kenwood', 'Oakland', \n",
    "    'South Shore', 'Washington Park', 'Woodlawn', 'Chatham', 'South Chicago', \n",
    "    'Auburn Gresham', 'Calumet Heights', 'Roseland', 'Pullman', 'West Pullman', \n",
    "    'Riverdale'\n",
    "]\n",
    "south_side_df = chicago_df[chicago_df['RegionName'].isin(south_side_regions)]\n",
    "\n",
    "# East Side\n",
    "east_side_regions = ['Hegewisch', 'East Side', 'South Shore', 'Hyde Park', 'Kenwood']\n",
    "east_side_df = chicago_df[chicago_df['RegionName'].isin(east_side_regions)]\n",
    "\n",
    "# West Side\n",
    "west_side_regions = [\n",
    "    'Austin', 'East Garfield Park', 'West Garfield Park', 'North Lawndale', \n",
    "    'South Lawndale (Little Village)', 'Humboldt Park', 'Near West Side', 'West Town'\n",
    "]\n",
    "west_side_df = chicago_df[chicago_df['RegionName'].isin(west_side_regions)]\n",
    "\n",
    "# Northwest Side\n",
    "northwest_side_regions = [\n",
    "    'Jefferson Park', 'Portage Park', 'Norwood Park', 'Dunning', 'Belmont Cragin', \n",
    "    'Montclare', 'Irving Park', 'Hermosa'\n",
    "]\n",
    "northwest_side_df = chicago_df[chicago_df['RegionName'].isin(northwest_side_regions)]\n",
    "\n",
    "# Southwest Side\n",
    "southwest_side_regions = [\n",
    "    'Garfield Ridge', 'Archer Heights', 'Brighton Park', 'Gage Park', 'West Elsdon', \n",
    "    'West Lawn', 'Chicago Lawn (Marquette Park)', 'Ashburn', 'Clearing'\n",
    "]\n",
    "southwest_side_df = chicago_df[chicago_df['RegionName'].isin(southwest_side_regions)]\n",
    "\n",
    "print(\"North Side neighborhoods: \\n\", north_side_df.head())\n",
    "print(\"South Side neighborhoods: \\n\", south_side_df.head())\n",
    "print(\"East Side neighborhoods: \\n\", east_side_df.head())\n",
    "print(\"West Side neighborhoods: \\n\", west_side_df.head())\n",
    "print(\"Northwest Side neighborhoods: \\n\", northwest_side_df.head())\n",
    "print(\"Southwest Side neighborhoods: \\n\", southwest_side_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivot date columns so data is more rectangular\n",
    "def reshape_dates(df):\n",
    "    # Identify date columns \n",
    "    date_columns = [col for col in df.columns if col.startswith('20')]\n",
    "    \n",
    "    # Melt dataframe to convert date columns into rows\n",
    "    df_melted = df.melt(id_vars=['RegionID', 'SizeRank', 'RegionName'],\n",
    "                        value_vars=date_columns,\n",
    "                        var_name='Date', value_name='ZHVI')\n",
    "    \n",
    "    # Convert Date to datetime fromat\n",
    "    df_melted['Date'] = pd.to_datetime(df_melted['Date'])\n",
    "\n",
    "    return df_melted\n",
    "\n",
    "reshaped_df = reshape_dates(chicago_df)\n",
    "reshaped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Lake View\n",
    "lake_view_df = reshaped_df[reshaped_df['RegionName'] == 'Lake View']\n",
    "\n",
    "# Plot ZHVI over time for Lake View\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Date', y='ZHVI', data=lake_view_df)\n",
    "plt.title(\"ZHVI Value Over Time for Lake View\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ZHVI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('Chicago_ZIP_Populations_2010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10616e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up populations csv's\n",
    "def clean_population(year):\n",
    "    filename = 'Chicago_ZIP_Populations_' + str(year) + '.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Transpose columns and rows\n",
    "    df = df.set_index('Label (Grouping)').transpose().reset_index()\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = ['Zip Code', 'Total']\n",
    "\n",
    "    # Remove prefix from Zip codes\n",
    "    df['Zip Code'] = df['Zip Code'].str[6:]\n",
    "    df['Year'] = year\n",
    "    \n",
    "    return df\n",
    "\n",
    "pop_2010 = clean_population(2010)\n",
    "pop_2020 = clean_population(2020)\n",
    "\n",
    "# Combine both years into one dataframe\n",
    "population_df = pd.concat([pop_2010, pop_2020], ignore_index=True)\n",
    "\n",
    "population_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# north side \n",
    "\n",
    "reshaped_north_side_df = reshape_dates(north_side_df)\n",
    "\n",
    "# Plot ZHVI over time for Lake View\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Date', y='ZHVI', data=reshaped_north_side_df)\n",
    "plt.title(\"ZHVI Value Over Time for North Side\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ZHVI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe783803",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neighborhoods_zillow_df = Neighborhoods_zillow_df[Neighborhoods_zillow_df['City'] == \"Chicago\"]\n",
    "Neighborhoods_zillow_df.to_csv('Chicago_Neighborhoods_Zillow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed616ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv('Chicago_Neighborhoods_Zillow.csv')\n",
    "\n",
    "# 2. Define the East Side regions\n",
    "east_side_regions = [\n",
    "    'Hegewisch', 'East Side', 'South Shore', 'Hyde Park', 'Kenwood'\n",
    "]\n",
    "\n",
    "# 3. Filter the DataFrame to include only the East Side regions\n",
    "df = df[df['RegionName'].isin(east_side_regions)]\n",
    "\n",
    "# 4. Reshape the data from wide to long format\n",
    "# Identify non-date columns\n",
    "non_date_cols = ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', \n",
    "                 'State', 'City', 'Metro', 'CountyName']\n",
    "\n",
    "# Reshape the DataFrame\n",
    "df_long = df.melt(id_vars=non_date_cols, var_name='Date', value_name='MedianHomeValue')\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "df_long['Date'] = pd.to_datetime(df_long['Date'])\n",
    "\n",
    "# 5. Handle missing values\n",
    "df_long.dropna(subset=['MedianHomeValue'], inplace=True)\n",
    "\n",
    "# 6. Feature engineering\n",
    "# Extract 'Year' and 'Month' from 'Date'\n",
    "df_long['Year'] = df_long['Date'].dt.year\n",
    "df_long['Month'] = df_long['Date'].dt.month\n",
    "\n",
    "# 7. Create lag features\n",
    "# Sort values\n",
    "df_long.sort_values(['RegionName', 'Date'], inplace=True)\n",
    "\n",
    "# Create lag features for the past 12 months\n",
    "for lag in range(1, 13):\n",
    "    df_long[f'Lag_{lag}'] = df_long.groupby('RegionName')['MedianHomeValue'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values due to lagging\n",
    "df_long.dropna(inplace=True)\n",
    "\n",
    "# 8. Encode categorical variables\n",
    "# One-hot encode 'RegionType' if necessary\n",
    "df_encoded = pd.get_dummies(df_long, columns=['RegionType'], drop_first=True)\n",
    "\n",
    "# 9. Define features and target variable\n",
    "# Lag features\n",
    "lag_features = [f'Lag_{lag}' for lag in range(1, 13)]\n",
    "# Categorical features (if any)\n",
    "categorical_features = [col for col in df_encoded.columns if col.startswith('RegionType_')]\n",
    "# All features\n",
    "features = lag_features + ['Year', 'Month'] + categorical_features\n",
    "\n",
    "X = df_encoded[features]\n",
    "y = df_encoded['MedianHomeValue']\n",
    "\n",
    "# 10. Split the data using TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# 11. Model training and selection\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Simplify hyperparameter tuning for quicker runtime\n",
    "param_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [200],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv,\n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# 12. Evaluate the model\n",
    "# Predictions on the training set\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Calculate RMSE \n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# 13. Forecast future values\n",
    "# Predict the next 12 months\n",
    "future_dates = pd.date_range(start=df_long['Date'].max() + pd.DateOffset(months=1),\n",
    "                             periods=12, freq='ME')  #change periods to 24 for 2 years\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_df = pd.DataFrame({'Date': future_dates})\n",
    "\n",
    "# Prepare future predictions for each region\n",
    "regions = df_long['RegionName'].unique()\n",
    "future_predictions_list = []  # Use a list to collect DataFrames\n",
    "\n",
    "for region in regions:\n",
    "    # Get the last known data for the region\n",
    "    region_data = df_encoded[df_encoded['RegionName'] == region].sort_values('Date')\n",
    "    last_row = region_data.iloc[-1]\n",
    "    \n",
    "    # Initialize lag values\n",
    "    lag_values = last_row[[f'Lag_{lag}' for lag in range(1, 13)]].values\n",
    "    # The most recent value is 'MedianHomeValue' from the last row\n",
    "    last_value = last_row['MedianHomeValue']\n",
    "    \n",
    "    # Create future data for the region\n",
    "    temp_df = future_df.copy()\n",
    "    temp_df['RegionName'] = region\n",
    "    temp_df['Year'] = temp_df['Date'].dt.year\n",
    "    temp_df['Month'] = temp_df['Date'].dt.month\n",
    "    \n",
    "    # Include any encoded 'RegionType' columns\n",
    "    for col in categorical_features:\n",
    "        temp_df[col] = last_row[col]\n",
    "    \n",
    "    # Initialize a DataFrame to store lag features\n",
    "    predicted_values = []\n",
    "    \n",
    "    for i in range(len(temp_df)):\n",
    "        # Create a dictionary to hold features for this date\n",
    "        features_dict = {}\n",
    "        # Set lag features\n",
    "        for lag in range(1, 13):\n",
    "            features_dict[f'Lag_{lag}'] = lag_values[lag-1]\n",
    "        # Combine features\n",
    "        features_input = pd.DataFrame(features_dict, index=[0])\n",
    "        features_input['Year'] = temp_df.iloc[i]['Year']\n",
    "        features_input['Month'] = temp_df.iloc[i]['Month']\n",
    "        for col in categorical_features:\n",
    "            features_input[col] = temp_df.iloc[i][col]\n",
    "        # Ensure all features are present\n",
    "        features_input = features_input[features]\n",
    "        # Predict the median home value\n",
    "        predicted_value = best_model.predict(features_input)[0]\n",
    "        predicted_values.append(predicted_value)\n",
    "        # Update lag values\n",
    "        lag_values = np.roll(lag_values, 1)\n",
    "        lag_values[0] = predicted_value  # The most recent lag is the predicted value\n",
    "    \n",
    "    # Add the predicted values to temp_df\n",
    "    temp_df['PredictedMedianHomeValue'] = predicted_values\n",
    "    \n",
    "    # Collect temp_df in the list\n",
    "    future_predictions_list.append(temp_df[['Date', 'RegionName', 'PredictedMedianHomeValue']])\n",
    "\n",
    "# Concatenate all future predictions into a single DataFrame\n",
    "future_predictions = pd.concat(future_predictions_list, ignore_index=True)\n",
    "\n",
    "# 14. Visualize the predictions\n",
    "# Select a region to visualize (e.g., 'East Side')\n",
    "region_to_plot = 'East Side'  # Replace with an actual region name from your data\n",
    "\n",
    "# Actual data\n",
    "actual_data = df_long[df_long['RegionName'] == region_to_plot]\n",
    "\n",
    "# Predicted data\n",
    "predicted_data = future_predictions[future_predictions['RegionName'] == region_to_plot]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(actual_data['Date'], actual_data['MedianHomeValue'], label='Actual')\n",
    "plt.plot(predicted_data['Date'], predicted_data['PredictedMedianHomeValue'],\n",
    "         label='Predicted', linestyle='--')\n",
    "plt.title(f'Median Home Value Trends for {region_to_plot}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Median Home Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv('Chicago_Neighborhoods_Zillow.csv')\n",
    "\n",
    "# 2. Reshape the data from wide to long format\n",
    "# Identify non-date columns\n",
    "non_date_cols = ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', \n",
    "                 'State', 'City', 'Metro', 'CountyName']\n",
    "\n",
    "# Reshape the DataFrame\n",
    "df_long = df.melt(id_vars=non_date_cols, var_name='Date', value_name='MedianHomeValue')\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "df_long['Date'] = pd.to_datetime(df_long['Date'])\n",
    "\n",
    "# 3. Handle missing values\n",
    "df_long.dropna(subset=['MedianHomeValue'], inplace=True)\n",
    "\n",
    "# 4. Aggregate the data across all regions\n",
    "# Calculate the average median home value for each date\n",
    "aggregate_data = df_long.groupby('Date')['MedianHomeValue'].mean().reset_index()\n",
    "\n",
    "# 5. Feature engineering\n",
    "# Extract 'Year' and 'Month' from 'Date'\n",
    "aggregate_data['Year'] = aggregate_data['Date'].dt.year\n",
    "aggregate_data['Month'] = aggregate_data['Date'].dt.month\n",
    "\n",
    "# 6. Create lag features\n",
    "# Sort values\n",
    "aggregate_data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Create lag features for the past 12 months\n",
    "for lag in range(1, 13):\n",
    "    aggregate_data[f'Lag_{lag}'] = aggregate_data['MedianHomeValue'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values due to lagging\n",
    "aggregate_data.dropna(inplace=True)\n",
    "\n",
    "# 7. Define features and target variable\n",
    "# Lag features\n",
    "lag_features = [f'Lag_{lag}' for lag in range(1, 13)]\n",
    "# All features\n",
    "features = lag_features + ['Year', 'Month']\n",
    "\n",
    "X = aggregate_data[features]\n",
    "y = aggregate_data['MedianHomeValue']\n",
    "\n",
    "# 8. Split the data using TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# 9. Model training and selection\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Adjust hyperparameters as needed\n",
    "param_grid = {\n",
    "    'n_estimators': [1000],  # Adjust based on your system's capacity\n",
    "    'max_depth': [200],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv,\n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# 10. Evaluate the model\n",
    "# Predictions on the training set\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# 11. Forecast future values\n",
    "# Predict the next 12 months\n",
    "future_dates = pd.date_range(start=aggregate_data['Date'].max() + pd.DateOffset(months=1),\n",
    "                             periods=12, freq='ME')  # Change periods to 24 for 2 years if needed\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_df = pd.DataFrame({'Date': future_dates})\n",
    "future_df['Year'] = future_df['Date'].dt.year\n",
    "future_df['Month'] = future_df['Date'].dt.month\n",
    "\n",
    "# Initialize lag values with the last available data\n",
    "lag_values = aggregate_data.iloc[-1][lag_features].values\n",
    "\n",
    "predicted_values = []\n",
    "\n",
    "for i in range(len(future_df)):\n",
    "    # Create a dictionary to hold features for this date\n",
    "    features_dict = {}\n",
    "    # Set lag features\n",
    "    for lag in range(1, 13):\n",
    "        features_dict[f'Lag_{lag}'] = lag_values[lag-1]\n",
    "    # Combine features\n",
    "    features_input = pd.DataFrame(features_dict, index=[0])\n",
    "    features_input['Year'] = future_df.iloc[i]['Year']\n",
    "    features_input['Month'] = future_df.iloc[i]['Month']\n",
    "    features_input = features_input[features]\n",
    "    # Predict the median home value\n",
    "    predicted_value = best_model.predict(features_input)[0]\n",
    "    predicted_values.append(predicted_value)\n",
    "    # Update lag values\n",
    "    lag_values = np.roll(lag_values, 1)\n",
    "    lag_values[0] = predicted_value  # The most recent lag is the predicted value\n",
    "\n",
    "# Add the predicted values to future_df\n",
    "future_df['PredictedMedianHomeValue'] = predicted_values\n",
    "\n",
    "# 12. Visualize the predictions\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(aggregate_data['Date'], aggregate_data['MedianHomeValue'], label='Actual')\n",
    "plt.plot(future_df['Date'], future_df['PredictedMedianHomeValue'],\n",
    "         label='Predicted', linestyle='--')\n",
    "plt.title('Median Home Value Trends for Chicago')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Median Home Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
