{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0de40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca7a0d",
   "metadata": {},
   "source": [
    "# Team 4\n",
    "## Real Estate Score Card\n",
    "\n",
    "The data we are analyzing is related to real estate prices from Zillow. The data is in the form of a CSV file, containing the following columns:\n",
    "- **RegionID**: Unique identifier for the region\n",
    "- **SizeRank**: Rank of the region based on size\n",
    "- **RegionName**: Name of the region\n",
    "- **RegionType**: Type of the region: neighborhood\n",
    "- **StateName**: Name of the state\n",
    "- **State**: Abbreviation of the state\n",
    "- **City**\n",
    "- **Metro**\n",
    "- **CountyName**\n",
    "- **Series of Dates from 2000-01 to 2024-09**, incremented per month\n",
    "\n",
    "The values associated we are looking at is called the Zillow Home Value Index (ZHVI).\n",
    "\n",
    "**Zillow Home Value Index (ZHVI)**: A measure of the typical home value and market changes across a given region and housing type. It reflects the typical value for homes in the 35th to 65th percentile range. More info about ZHVI: [Zillow Research](https://www.zillow.com/research/methodology-neural-zhvi-32128/)\n",
    "\n",
    "Our analysis will be focused on the Chicago land area consisting of all its neighborhoods and regions associated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "df = pd.read_csv('Neighborhood_zillow.csv')\n",
    "Neighborhoods_zillow_df = df.copy()\n",
    "print(\"shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b90ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data to only include chicago neighborhoods\n",
    "chicago_df = df[df['City'] == 'Chicago']\n",
    "\n",
    "# List the neighborhoods\n",
    "\n",
    "print(\"Number of unique region names in Chicago:\", df['RegionName'].unique().size)\n",
    "\n",
    "# Remove the columns that are not needed\n",
    "chicago_df = chicago_df.drop(columns=['StateName', 'Metro', 'CountyName'])\n",
    "\n",
    "# Save the df into a csv for chicago\n",
    "chicago_df.to_csv('Chicago_Neighborhoods.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b948905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter regions into unique dataframes\n",
    "\n",
    "# North Side\n",
    "north_side_regions = [\n",
    "    'Rogers Park', 'Edgewater', 'Uptown', 'Lake View', 'Lincoln Park', \n",
    "    'North Center', 'Lincoln Square', 'West Ridge', 'Irving Park', \n",
    "    'Albany Park', 'Avondale'\n",
    "]\n",
    "north_side_df = chicago_df[chicago_df['RegionName'].isin(north_side_regions)]\n",
    "\n",
    "# South Side\n",
    "south_side_regions = [\n",
    "    'Armour Square', 'Bridgeport', 'Brighton Park', 'New City (Back of the Yards)', \n",
    "    'Englewood', 'Greater Grand Crossing', 'Hyde Park', 'Kenwood', 'Oakland', \n",
    "    'South Shore', 'Washington Park', 'Woodlawn', 'Chatham', 'South Chicago', \n",
    "    'Auburn Gresham', 'Calumet Heights', 'Roseland', 'Pullman', 'West Pullman', \n",
    "    'Riverdale'\n",
    "]\n",
    "south_side_df = chicago_df[chicago_df['RegionName'].isin(south_side_regions)]\n",
    "\n",
    "# East Side\n",
    "east_side_regions = [\n",
    "    'Hegewisch', 'East Side', 'South Shore', 'Hyde Park', 'Kenwood'\n",
    "    \n",
    "]\n",
    "east_side_df = chicago_df[chicago_df['RegionName'].isin(east_side_regions)]\n",
    "\n",
    "# West Side\n",
    "west_side_regions = [\n",
    "    'Austin', 'East Garfield Park', 'West Garfield Park', 'North Lawndale', \n",
    "    'South Lawndale (Little Village)', 'Humboldt Park', 'Near West Side', 'West Town'\n",
    "]\n",
    "west_side_df = chicago_df[chicago_df['RegionName'].isin(west_side_regions)]\n",
    "\n",
    "# Northwest Side\n",
    "northwest_side_regions = [\n",
    "    'Jefferson Park', 'Portage Park', 'Norwood Park', 'Dunning', 'Belmont Cragin', \n",
    "    'Montclare', 'Irving Park', 'Hermosa'\n",
    "]\n",
    "northwest_side_df = chicago_df[chicago_df['RegionName'].isin(northwest_side_regions)]\n",
    "\n",
    "# Southwest Side\n",
    "southwest_side_regions = [\n",
    "    'Garfield Ridge', 'Archer Heights', 'Brighton Park', 'Gage Park', 'West Elsdon', \n",
    "    'West Lawn', 'Chicago Lawn (Marquette Park)', 'Ashburn', 'Clearing'\n",
    "]\n",
    "southwest_side_df = chicago_df[chicago_df['RegionName'].isin(southwest_side_regions)]\n",
    "\n",
    "print(\"North Side neighborhoods: \\n\", north_side_df.head(3))\n",
    "# print(\"South Side neighborhoods: \\n\", south_side_df.head(8))\n",
    "# print(\"East Side neighborhoods: \\n\", east_side_df.head(8))\n",
    "# print(\"West Side neighborhoods: \\n\", west_side_df.head(8))\n",
    "# print(\"Northwest Side neighborhoods: \\n\", northwest_side_df.head(8))\n",
    "# print(\"Southwest Side neighborhoods: \\n\", southwest_side_df.head(8))\n",
    "\n",
    "# print(\"North Side neighborhoods: \\n\", north_side_df.head())\n",
    "# print(\"South Side neighborhoods: \\n\", south_side_df.head())\n",
    "# print(\"East Side neighborhoods: \\n\", east_side_df.head())\n",
    "# print(\"West Side neighborhoods: \\n\", west_side_df.head())\n",
    "# print(\"Northwest Side neighborhoods: \\n\", northwest_side_df.head())\n",
    "# print(\"Southwest Side neighborhoods: \\n\", southwest_side_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivot date columns so data is more rectangular\n",
    "def reshape_dates(df):\n",
    "    # Identify date columns \n",
    "    date_columns = [col for col in df.columns if col.startswith('20')]\n",
    "    \n",
    "    # Melt dataframe to convert date columns into rows\n",
    "    df_melted = df.melt(id_vars=['RegionID', 'SizeRank', 'RegionName'],\n",
    "                        value_vars=date_columns,\n",
    "                        var_name='Date', value_name='ZHVI')\n",
    "    \n",
    "    # Convert Date to datetime fromat\n",
    "    df_melted['Date'] = pd.to_datetime(df_melted['Date'])\n",
    "\n",
    "    return df_melted\n",
    "\n",
    "reshaped_df = reshape_dates(chicago_df)\n",
    "reshaped_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Lake View\n",
    "lake_view_df = reshaped_df[reshaped_df['RegionName'] == 'Lake View']\n",
    "\n",
    "# Plot ZHVI over time for Lake View\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Date', y='ZHVI', data=lake_view_df)\n",
    "plt.title(\"ZHVI Value Over Time for Lake View\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ZHVI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cens = pd.read_csv('Data/Census-2020.csv')\n",
    "cens['Label (Grouping)'].to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_census(year):\n",
    "    filename = 'Data/Census-' + str(year) + '.csv'\n",
    "    raw_df = pd.read_csv(filename)\n",
    "\n",
    "    # Filter only Total Estimate columns\n",
    "    columns = raw_df.columns.str\n",
    "    df = raw_df.loc[:, columns.contains('Total!!Estimate')]\n",
    "    df.loc[:,'Label'] = raw_df['Label (Grouping)']\n",
    "\n",
    "    # Transpose columns and rows\n",
    "    df = df.set_index('Label').transpose().reset_index()\n",
    "\n",
    "    # Rename columns that have Year in the name\n",
    "    df = df.rename(columns={'index': 'ZIP Code', \n",
    "                            'INCOME IN THE PAST 12 MONTHS (IN ' + str(year) + ' INFLATION-ADJUSTED DOLLARS)':\n",
    "                            'INCOME IN THE PAST 12 MONTHS (IN INFLATION-ADJUSTED DOLLARS)', \n",
    "                            'EARNINGS IN THE PAST 12 MONTHS (IN ' + str(year) + ' INFLATION-ADJUSTED DOLLARS) FOR FULL-TIME, YEAR-ROUND WORKERS': \n",
    "                            'EARNINGS IN THE PAST 12 MONTHS (IN INFLATION-ADJUSTED DOLLARS) FOR FULL-TIME, YEAR-ROUND WORKERS'})\n",
    "    \n",
    "    \n",
    "    # Rename columns so each is unique\n",
    "    new_columns = []\n",
    "    prefix = ''\n",
    "\n",
    "    for col in df.columns:\n",
    "        col = col.replace('\\xa0', '')\n",
    "        if col.isupper():\n",
    "            # If the column name is all uppercase, set it as the prefix\n",
    "            prefix = col\n",
    "            new_columns.append(col)\n",
    "        else:\n",
    "            # Add the prefix to the column name\n",
    "            new_columns.append(f\"{prefix} - {col}\")\n",
    "    \n",
    "    df.columns = new_columns\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Remove columns with all NaN values\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # This column was causing trouble so remove it\n",
    "    for col in df.columns:\n",
    "        if 'with related children' in col.lower():\n",
    "            df.drop(columns=[col])\n",
    "\n",
    "    df = df.rename(columns={'- ZIP Code': 'ZIP Code', '- Total population': 'Total population'})\n",
    "    df['ZIP Code'] = df['ZIP Code'].str[6:11]\n",
    "\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Check if the column is of type 'object'\n",
    "            if df[col].dtype == 'object':\n",
    "\n",
    "                # Remove commas\n",
    "                df[col] = df[col].replace({',': ''}, regex=True)\n",
    "                \n",
    "                # Check if the column contains percentages\n",
    "                if df[col].str.contains('%').any():\n",
    "                    # Convert to float after removing % sign and divide by 100\n",
    "                    df[col] = df[col].str.rstrip('%').astype('float') / 100\n",
    "                else:\n",
    "                    df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "        except Exception as e:\n",
    "            nonee = ''\n",
    "\n",
    "    # Clean up column names and add year column\n",
    "    df['Year'] = year\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "census_df = clean_census(2011)\n",
    "\n",
    "# Loop through each year and add it to the census_df\n",
    "for year in range(2012,2023):\n",
    "    df = clean_census(year)\n",
    "\n",
    "    # Remove columns not in the intersection so they can be concated\n",
    "    common_columns = census_df.columns.intersection(df.columns)\n",
    "    census_df = census_df[common_columns]\n",
    "    df = df[common_columns]\n",
    "\n",
    "    census_df = pd.concat([census_df, df], ignore_index=True)\n",
    "\n",
    "census_df['ZIP Code'] = census_df['ZIP Code'].astype(str)\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "census_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c23aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(census_df.dtypes)\n",
    "# Group by Zip and get average of population\n",
    "grouped_df = census_df.groupby('ZIP Code')['Total population'].mean().reset_index()\n",
    "\n",
    "# Get the top 10 ZIP codes by average population\n",
    "top_10 = grouped_df.sort_values('Total population', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='ZIP Code', y='Total population', data=top_10)\n",
    "plt.title('Top 10 ZIP Codes by Average Total Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3dc8e3",
   "metadata": {},
   "source": [
    "This visualization shows the top 10 ZIP codes by average population, highlighting areas with the highest population density, which can indicate regions with greater housing demand, community resources, and potential market opportunities for real estate investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10616e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up populations csv's\n",
    "def clean_population(year):\n",
    "    filename = 'Chicago_ZIP_Populations_' + str(year) + '.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Transpose columns and rows\n",
    "    df = df.set_index('Label (Grouping)').transpose().reset_index()\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = ['Zip Code', 'Total']\n",
    "\n",
    "    # Remove prefix from Zip codes\n",
    "    df['Zip Code'] = df['Zip Code'].str[6:]\n",
    "    df['Year'] = year\n",
    "    \n",
    "    return df\n",
    "\n",
    "pop_2010 = clean_population(2010)\n",
    "pop_2020 = clean_population(2020)\n",
    "\n",
    "# Combine both years into one dataframe\n",
    "population_df = pd.concat([pop_2010, pop_2020], ignore_index=True)\n",
    "\n",
    "population_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# north side \n",
    "\n",
    "reshaped_north_side_df = reshape_dates(north_side_df)\n",
    "\n",
    "# print (\"North Side neighborhoods: \\n\", reshaped_north_side_df[\"RegionName\"].unique())\n",
    "# print (\"North Side neighborhoods: \\n\", reshaped_north_side_df.columns)\n",
    "\n",
    "# Ensure Date column is in datetime format\n",
    "reshaped_north_side_df['Date'] = pd.to_datetime(reshaped_north_side_df['Date'])\n",
    "\n",
    "# Plot ZHVI over time for Lake View\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Date', y='ZHVI', data=reshaped_north_side_df)\n",
    "plt.title(\"ZHVI Value Over Time for North Side (Combined)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ZHVI\")\n",
    "plt.show()\n",
    "\n",
    "# 1. ZHVI Over Time for North Side - Line Graph\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=reshaped_north_side_df, x='Date', y='ZHVI', hue='RegionName')\n",
    "plt.title('ZHVI Over Time for North Side Regions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('ZHVI')\n",
    "plt.legend(title='Region')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Average ZHVI by Region - Bar Chart\n",
    "average_zhvi_by_region = reshaped_north_side_df.groupby('RegionName')['ZHVI'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=average_zhvi_by_region, x='RegionName', y='ZHVI', palette='viridis')\n",
    "plt.title('Average ZHVI by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average ZHVI')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. ZHVI Distribution by Region - Box Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=reshaped_north_side_df, x='RegionName', y='ZHVI', palette='pastel')\n",
    "plt.title('ZHVI Distribution by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('ZHVI')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e586cc5",
   "metadata": {},
   "source": [
    "ZHVI Value Over Time for North Side (Combined) - This visualization provides a broad view of the overall Zillow Home Value Index (ZHVI) trends across all neighborhoods in the North Side, giving a quick overview of how property values have evolved in this region as a whole.\n",
    "\n",
    "ZHVI Over Time for North Side Regions - By breaking down ZHVI trends for individual neighborhoods within the North Side, this graph allows for a more detailed comparison, helping identify which neighborhoods have higher growth rates or stability over time.\n",
    "\n",
    "Average ZHVI by Region - Bar Chart - This bar chart highlights the average ZHVI across North Side neighborhoods, offering a clear comparison of property values among neighborhoods, which is useful for assessing relative affordability and investment potential.\n",
    "\n",
    "ZHVI Distribution by Region - Box Plot - The box plot shows the range and variability of ZHVI within each neighborhood, illustrating price distribution and identifying areas with higher volatility or consistency in home values, which can indicate market stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe783803",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neighborhoods_zillow_df = Neighborhoods_zillow_df[Neighborhoods_zillow_df['City'] == \"Chicago\"]\n",
    "Neighborhoods_zillow_df.to_csv('Chicago_Neighborhoods_Zillow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed616ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv('Chicago_Neighborhoods_Zillow.csv')\n",
    "\n",
    "# 2. Define the East Side regions\n",
    "east_side_regions = [\n",
    "    'Hegewisch', 'East Side', 'South Shore', 'Hyde Park', 'Kenwood'\n",
    "]\n",
    "\n",
    "# 3. Filter the DataFrame to include only the East Side regions\n",
    "df = df[df['RegionName'].isin(east_side_regions)]\n",
    "\n",
    "# 4. Reshape the data from wide to long format\n",
    "# Identify non-date columns\n",
    "non_date_cols = ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', \n",
    "                 'State', 'City', 'Metro', 'CountyName']\n",
    "\n",
    "# Reshape the DataFrame\n",
    "df_long = df.melt(id_vars=non_date_cols, var_name='Date', value_name='MedianHomeValue')\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "df_long['Date'] = pd.to_datetime(df_long['Date'])\n",
    "\n",
    "# 5. Handle missing values\n",
    "df_long.dropna(subset=['MedianHomeValue'], inplace=True)\n",
    "\n",
    "# 6. Feature engineering\n",
    "# Extract 'Year' and 'Month' from 'Date'\n",
    "df_long['Year'] = df_long['Date'].dt.year\n",
    "df_long['Month'] = df_long['Date'].dt.month\n",
    "\n",
    "# 7. Create lag features\n",
    "# Sort values\n",
    "df_long.sort_values(['RegionName', 'Date'], inplace=True)\n",
    "\n",
    "# Create lag features for the past 12 months\n",
    "for lag in range(1, 13):\n",
    "    df_long[f'Lag_{lag}'] = df_long.groupby('RegionName')['MedianHomeValue'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values due to lagging\n",
    "df_long.dropna(inplace=True)\n",
    "\n",
    "# 8. Encode categorical variables\n",
    "# One-hot encode 'RegionType' if necessary\n",
    "df_encoded = pd.get_dummies(df_long, columns=['RegionType'], drop_first=True)\n",
    "\n",
    "# 9. Define features and target variable\n",
    "# Lag features\n",
    "lag_features = [f'Lag_{lag}' for lag in range(1, 13)]\n",
    "# Categorical features (if any)\n",
    "categorical_features = [col for col in df_encoded.columns if col.startswith('RegionType_')]\n",
    "# All features\n",
    "features = lag_features + ['Year', 'Month'] + categorical_features\n",
    "\n",
    "X = df_encoded[features]\n",
    "y = df_encoded['MedianHomeValue']\n",
    "\n",
    "# 10. Split the data using TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# 11. Model training and selection\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Simplify hyperparameter tuning for quicker runtime\n",
    "param_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [200],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv,\n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# 12. Evaluate the model\n",
    "# Predictions on the training set\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Calculate RMSE \n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# 13. Forecast future values\n",
    "# Predict the next 12 months\n",
    "future_dates = pd.date_range(start=df_long['Date'].max() + pd.DateOffset(months=1),\n",
    "                             periods=12, freq='ME')  #change periods to 24 for 2 years\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_df = pd.DataFrame({'Date': future_dates})\n",
    "\n",
    "# Prepare future predictions for each region\n",
    "regions = df_long['RegionName'].unique()\n",
    "future_predictions_list = []  # Use a list to collect DataFrames\n",
    "\n",
    "for region in regions:\n",
    "    # Get the last known data for the region\n",
    "    region_data = df_encoded[df_encoded['RegionName'] == region].sort_values('Date')\n",
    "    last_row = region_data.iloc[-1]\n",
    "    \n",
    "    # Initialize lag values\n",
    "    lag_values = last_row[[f'Lag_{lag}' for lag in range(1, 13)]].values\n",
    "    # The most recent value is 'MedianHomeValue' from the last row\n",
    "    last_value = last_row['MedianHomeValue']\n",
    "    \n",
    "    # Create future data for the region\n",
    "    temp_df = future_df.copy()\n",
    "    temp_df['RegionName'] = region\n",
    "    temp_df['Year'] = temp_df['Date'].dt.year\n",
    "    temp_df['Month'] = temp_df['Date'].dt.month\n",
    "    \n",
    "    # Include any encoded 'RegionType' columns\n",
    "    for col in categorical_features:\n",
    "        temp_df[col] = last_row[col]\n",
    "    \n",
    "    # Initialize a DataFrame to store lag features\n",
    "    predicted_values = []\n",
    "    \n",
    "    for i in range(len(temp_df)):\n",
    "        # Create a dictionary to hold features for this date\n",
    "        features_dict = {}\n",
    "        # Set lag features\n",
    "        for lag in range(1, 13):\n",
    "            features_dict[f'Lag_{lag}'] = lag_values[lag-1]\n",
    "        # Combine features\n",
    "        features_input = pd.DataFrame(features_dict, index=[0])\n",
    "        features_input['Year'] = temp_df.iloc[i]['Year']\n",
    "        features_input['Month'] = temp_df.iloc[i]['Month']\n",
    "        for col in categorical_features:\n",
    "            features_input[col] = temp_df.iloc[i][col]\n",
    "        # Ensure all features are present\n",
    "        features_input = features_input[features]\n",
    "        # Predict the median home value\n",
    "        predicted_value = best_model.predict(features_input)[0]\n",
    "        predicted_values.append(predicted_value)\n",
    "        # Update lag values\n",
    "        lag_values = np.roll(lag_values, 1)\n",
    "        lag_values[0] = predicted_value  # The most recent lag is the predicted value\n",
    "    \n",
    "    # Add the predicted values to temp_df\n",
    "    temp_df['PredictedMedianHomeValue'] = predicted_values\n",
    "    \n",
    "    # Collect temp_df in the list\n",
    "    future_predictions_list.append(temp_df[['Date', 'RegionName', 'PredictedMedianHomeValue']])\n",
    "\n",
    "# Concatenate all future predictions into a single DataFrame\n",
    "future_predictions = pd.concat(future_predictions_list, ignore_index=True)\n",
    "\n",
    "# 14. Visualize the predictions\n",
    "# Select a region to visualize (e.g., 'East Side')\n",
    "region_to_plot = 'East Side'  # Replace with an actual region name from your data\n",
    "\n",
    "# Actual data\n",
    "actual_data = df_long[df_long['RegionName'] == region_to_plot]\n",
    "\n",
    "# Predicted data\n",
    "predicted_data = future_predictions[future_predictions['RegionName'] == region_to_plot]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(actual_data['Date'], actual_data['MedianHomeValue'], label='Actual')\n",
    "plt.plot(predicted_data['Date'], predicted_data['PredictedMedianHomeValue'],\n",
    "         label='Predicted', linestyle='--')\n",
    "plt.title(f'Median Home Value Trends for {region_to_plot}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Median Home Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b891dd9",
   "metadata": {},
   "source": [
    "This visualization is beneficial as it focuses on the East Side regions, allowing for a detailed view of historical and predicted home value trends in a specific area, which is useful for regional market analysis and targeted investment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv('Chicago_Neighborhoods_Zillow.csv')\n",
    "\n",
    "# 2. Reshape the data from wide to long format\n",
    "# Identify non-date columns\n",
    "non_date_cols = ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', \n",
    "                 'State', 'City', 'Metro', 'CountyName']\n",
    "\n",
    "# Reshape the DataFrame\n",
    "df_long = df.melt(id_vars=non_date_cols, var_name='Date', value_name='MedianHomeValue')\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "df_long['Date'] = pd.to_datetime(df_long['Date'])\n",
    "\n",
    "# 3. Handle missing values\n",
    "df_long.dropna(subset=['MedianHomeValue'], inplace=True)\n",
    "\n",
    "# 4. Aggregate the data across all regions\n",
    "# Calculate the average median home value for each date\n",
    "aggregate_data = df_long.groupby('Date')['MedianHomeValue'].mean().reset_index()\n",
    "\n",
    "# 5. Feature engineering\n",
    "# Extract 'Year' and 'Month' from 'Date'\n",
    "aggregate_data['Year'] = aggregate_data['Date'].dt.year\n",
    "aggregate_data['Month'] = aggregate_data['Date'].dt.month\n",
    "\n",
    "# 6. Create lag features\n",
    "# Sort values\n",
    "aggregate_data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Create lag features for the past 12 months\n",
    "for lag in range(1, 13):\n",
    "    aggregate_data[f'Lag_{lag}'] = aggregate_data['MedianHomeValue'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values due to lagging\n",
    "aggregate_data.dropna(inplace=True)\n",
    "\n",
    "# 7. Define features and target variable\n",
    "# Lag features\n",
    "lag_features = [f'Lag_{lag}' for lag in range(1, 13)]\n",
    "# All features\n",
    "features = lag_features + ['Year', 'Month']\n",
    "\n",
    "X = aggregate_data[features]\n",
    "y = aggregate_data['MedianHomeValue']\n",
    "\n",
    "# 8. Split the data using TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# 9. Model training and selection\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Adjust hyperparameters as needed\n",
    "param_grid = {\n",
    "    'n_estimators': [1000],  # Adjust based on your system's capacity\n",
    "    'max_depth': [200],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv,\n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# 10. Evaluate the model\n",
    "# Predictions on the training set\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# 11. Forecast future values\n",
    "# Predict the next 12 months\n",
    "future_dates = pd.date_range(start=aggregate_data['Date'].max() + pd.DateOffset(months=1),\n",
    "                             periods=12, freq='ME')  # Change periods to 24 for 2 years if needed\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_df = pd.DataFrame({'Date': future_dates})\n",
    "future_df['Year'] = future_df['Date'].dt.year\n",
    "future_df['Month'] = future_df['Date'].dt.month\n",
    "\n",
    "# Initialize lag values with the last available data\n",
    "lag_values = aggregate_data.iloc[-1][lag_features].values\n",
    "\n",
    "predicted_values = []\n",
    "\n",
    "for i in range(len(future_df)):\n",
    "    # Create a dictionary to hold features for this date\n",
    "    features_dict = {}\n",
    "    # Set lag features\n",
    "    for lag in range(1, 13):\n",
    "        features_dict[f'Lag_{lag}'] = lag_values[lag-1]\n",
    "    # Combine features\n",
    "    features_input = pd.DataFrame(features_dict, index=[0])\n",
    "    features_input['Year'] = future_df.iloc[i]['Year']\n",
    "    features_input['Month'] = future_df.iloc[i]['Month']\n",
    "    features_input = features_input[features]\n",
    "    # Predict the median home value\n",
    "    predicted_value = best_model.predict(features_input)[0]\n",
    "    predicted_values.append(predicted_value)\n",
    "    # Update lag values\n",
    "    lag_values = np.roll(lag_values, 1)\n",
    "    lag_values[0] = predicted_value  # The most recent lag is the predicted value\n",
    "\n",
    "# Add the predicted values to future_df\n",
    "future_df['PredictedMedianHomeValue'] = predicted_values\n",
    "\n",
    "# 12. Visualize the predictions\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(aggregate_data['Date'], aggregate_data['MedianHomeValue'], label='Actual')\n",
    "plt.plot(future_df['Date'], future_df['PredictedMedianHomeValue'],\n",
    "         label='Predicted', linestyle='--')\n",
    "plt.title('Median Home Value Trends for Chicago')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Median Home Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872ddea",
   "metadata": {},
   "source": [
    "This graph is valuable because it displays both the historical trends in median home values for Chicago and the model’s future predictions, providing a clear visual representation of past market behavior and expected future trends, which is useful for understanding housing market dynamics and informing investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb3594",
   "metadata": {},
   "source": [
    "<h2>ML Analysis</h2>\n",
    "\n",
    "\n",
    "In the second graph titled __Median Home Value Trends for Chicago__, the line shows the median home value for all neighborhoods from around 2001 to 2024 and then we used a model to predict the home values for the future. The model predicts the median home value in Chicago to remain fairly constant with no noticable increase or decrease.\n",
    "\n",
    "\n",
    "However in the first graph titled __Median Home Value Trends for East Side__ we perform the same ML analysis on just the East Side neighborhood of Chicago. The model predicts a significant increase in the median home value for houses in the East Side. Given the fact that the median home value for across neighborhoods in Chicago is projected to remain pretty constant, we can assume that the East Side far outperforms the rest of the other neighborhoods since there is a significant increase compared to the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376609e7",
   "metadata": {},
   "source": [
    "    •    Data cleaning: show clearly how you cleaned your data.\n",
    "\n",
    "We collected data from Zillow for home value estimations and Census data for data about the people in each Chicago ZIP Code. To clean the Zillow data we had to transpose both data frames so that each ZIP Code was part of the row instead of a column. Then we had to remove the columns that weren’t total estimates. The original data set had nested data so we had to rename the columns to add the top level column name to the lower levels so that each column had a unique name. Then we iterated through the columns and converted to numeric data types based on whether the data was a normal number or a percentage. Next we added a Year column to each dataframe and combined all the dataframes into one, dropping any column that would stop the data frames from being concatenated. \n",
    "\n",
    "\n",
    "    •    Exploratory data analysis: explain what your data looks like (words are fine, but visualizations are often better). Include any interesting issues or preliminary conclusions you have about your data.\n",
    "\n",
    "The data for Zillow is organized by Neighborhood and ZHVI which is Zillows house value estimation. The Census data is much more vast and can be used in a multitude of ways. Some of the most notable are age, educational attainment, income level, and household size. With this data we were able to assess what types of people are congregating in which areas and how their house values correlate with the people in the zip code. We concluded that higher income residents that have higher education live in areas with higher average ZHVI. One issue that we will have to solve is accurately mapping the ZIP Codes to the neighborhood values that the Zillow dataset uses. Once we do that it will be much easier to do accurate data analysis regarding census and ZHVI data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a322e3",
   "metadata": {},
   "source": [
    "## Peer Assessment\n",
    "\n",
    "### Contributions and Roles\n",
    "\n",
    "- **Adam Nimer**: Spearheaded the EDA. Added Visualizations and justifications. **10/10**\n",
    "\n",
    "- **Daniel Mroz**: Performed Analysis of ML model and provided justification for results provided by the model. **10/10**\n",
    "\n",
    "- **Abdullah Ali**: Data Collection and Cleaning as well as validation of data sources. **10/10**\n",
    "\n",
    "- **Umar Ahmed Khan**: Data Cleaning, initial setup of data for EDA. **10/10**\n",
    "\n",
    "- **Bilal Naseer**: ML Model setup and execution. **10/10**\n",
    "\n",
    "- **Suraj Pillarisetti**: Did not contribute any commits to the GitHub repository or provide any insights on project goals. Regurgitated information already presented by Adam. **1/10**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
